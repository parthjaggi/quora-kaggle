{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai.learner import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  \u001b[01;34mtmp\u001b[0m/  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "PATH='data/quora/'\n",
    "\n",
    "F_NAME = 'train.csv'\n",
    "TRN = f'{PATH}{F_NAME}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What would a Trump presidency mean for current international master’s students on an F1 visa?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = df_train.question1[15]; ques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many words are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7681017\r\n"
     ]
    }
   ],
   "source": [
    "!find {TRN} -name '*.csv' | xargs cat | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size seems large enough to create our own language model.\n",
    "Even though this was not part of the problem, I felt this would make the final solution more comprehensive.\n",
    "The idea was to use a pre-trained language model, and another model trained on given data itself.\n",
    "Then comparing which of them works better in the Siamese model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words need to be tokenized, and tokenizer from fastai library is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What would a Trump presidency mean for current international master ’s students on an F1 visa ?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(spacy_tok(ques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is first question as read by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText library is used to preprocess data, and spacy for tokenization.\n",
    "\n",
    "'TEXT' field below (from TorchText) will contain all the vocabulary of our dataset. We configure it for lowercase and tokenize using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will creat a df_lang_model dataframe for training language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727722, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_series = df_train.question1.append(df_train.question2, ignore_index=True)\n",
    "df_lang_model = pd.DataFrame(ques_series, columns=['questions'])\n",
    "df_lang_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VAL_RATIO = 0.2\n",
    "\n",
    "# TODO: create a function for this\n",
    "idx = np.arange(df_lang_model.shape[0])\n",
    "np.random.seed(999)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "val_size = int(len(idx) * VAL_RATIO)\n",
    "\n",
    "df_lang_model_train = df_lang_model.iloc[idx[val_size:]]\n",
    "df_lang_model_val = df_lang_model.iloc[idx[:val_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a model-data object from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lang_model_test = df_lang_model_val.tail();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs=16; bptt=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bptt is the number of the words that we want our model to read at once.\n",
    "Both bs and bptt increase need for GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md = LanguageModelData.from_dataframes(PATH, TEXT, 'questions', df_lang_model_train, df_lang_model_val, df_lang_model_test, \n",
    "                                       bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above method fills the 'TEXT' object with vocab attribute. This stores words and integer (token) mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# storing TEXT object\n",
    "# pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, in trn_ds attribute we have dataset, and in trn_dl is the data loader for feeding into the optimizer.\n",
    "\n",
    "Also below are the: # batches; # unique tokens in the vocab; # tokens in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1774, 83932, 1, 7952092)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl), md.nt, len(md.trn_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer to String mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '?',\n",
       " '<eos>',\n",
       " 'the',\n",
       " 'what',\n",
       " 'is',\n",
       " 'i',\n",
       " 'how',\n",
       " 'a',\n",
       " 'to',\n",
       " 'in']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'itos': 'int-to-string'\n",
    "TEXT.vocab.itos[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'stoi': 'string to int'\n",
    "TEXT.vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'western',\n",
       " 'married',\n",
       " 'life',\n",
       " ',',\n",
       " 'does',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'have',\n",
       " 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0].text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText will handle turning words into integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "   11\n",
       "  993\n",
       "  744\n",
       "   73\n",
       "   18\n",
       "   25\n",
       "    4\n",
       "  979\n",
       "   32\n",
       "   10\n",
       "[torch.cuda.LongTensor of size 10x1 (GPU 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.numericalize([md.trn_ds[0].text[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'LanguageModelData' object or 'md' will create 16 batches with approximate lengths of 60 tokens (bptt).\n",
    "\n",
    "Below is 1 batch of data\n",
    "\n",
    "Each batch also has labels (or next word in this case of language model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "     11      7     57  ...       2     80   1150\n",
       "    993     80     21  ...       3      5    589\n",
       "    744     30      2  ...       5   3061      2\n",
       "         ...            ⋱           ...         \n",
       "     18      7     11  ...      19  46937  81205\n",
       "   2037    191   3856  ...      32     49     49\n",
       "     15      4      2  ...    1080    804      2\n",
       " [torch.cuda.LongTensor of size 68x64 (GPU 0)], Variable containing:\n",
       "    993\n",
       "     80\n",
       "     21\n",
       "   ⋮   \n",
       "     90\n",
       "    499\n",
       "      3\n",
       " [torch.cuda.LongTensor of size 4352 (GPU 0)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innitially generic model parameters are choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "em_sz = 200  # size of each embedding vector\n",
    "nh = 500     # number of hidden activations per layer\n",
    "nl = 3       # number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large momentum values don't work well with RNN models, so we have adam optimizer with less momentum than its \n",
    "default of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below get_model method gives us a variant of AWD LSTM Language Model. This model provides good regularization through dropout.\n",
    "\n",
    "Though dropout values for the model needs to be guessed and further optimized through training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = md.get_model(opt_fn, em_sz, nh, nl,\n",
    "               dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)\n",
    "learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learner.clip=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is slowly tuned in stages, learning rate finder could also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60355bf225594ae4b64ed3351061f0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      3.876628   3.765307  \n",
      "    1      3.667433   3.563167                                \n",
      "    2      3.509317   3.460986                                \n",
      "    3      3.612688   3.51687                                 \n",
      "    4      3.490294   3.422421                                \n",
      "    5      3.401346   3.349517                                \n",
      "    6      3.341496   3.329137                                \n",
      "    7      3.546147   3.463429                                \n",
      "    8      3.513475   3.428641                                \n",
      "    9      3.460858   3.385919                                \n",
      "    10     3.414455   3.343821                                \n",
      "    11     3.354729   3.301025                                \n",
      "    12     3.332077   3.270087                                \n",
      "    13     3.256523   3.251745                                \n",
      "    14     3.220865   3.251471                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.2514706]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('adam1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.load_encoder('adam1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15da4b717734d83a91522f45e4f0316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      3.478428   3.410003  \n",
      "    1      3.475994   3.39572                                 \n",
      "    2      3.475303   3.368326                                \n",
      "    3      3.408111   3.342648                                \n",
      "    4      3.366519   3.310779                                \n",
      "    5      3.320632   3.27357                                 \n",
      "    6      3.267667   3.247919                                \n",
      "    7      3.231382   3.228729                                \n",
      "    8      3.227391   3.218729                                \n",
      "    9      3.167791   3.21881                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.21881]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For question comparison, we only need half of the language model- the *encoder*, so we save that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.save_encoder('adam_20_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.load_encoder('adam_20_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the language model a bit, by giving it a small string and see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  How to'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=learner.model\n",
    "ss=\"\"\" How to \"\"\"\n",
    "s = [spacy_tok(ss)]\n",
    "t=TEXT.numericalize(s)\n",
    "' '.join(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll manually test the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m[0].bs=1          # set batch size to 1\n",
    "\n",
    "m.eval()           # turn off dropout\n",
    "\n",
    "m.reset()          # reset hidden state\n",
    "\n",
    "res,*_ = m(t)      # get predictions from model\n",
    "\n",
    "m[0].bs=bs         # put the batch size back to what it was"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next top 10 predictions were for the next word after our short text are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'a', 'be', 'get', 'make', 'my', 'an', 'work', 'your', 'india']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nexts = torch.topk(res[-1], 10)[1]\n",
    "[TEXT.vocab.itos[o] for o in to_np(nexts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a bit more text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How to  \n",
      "\n",
      "way to learn about stock market ? <eos> what is the best way to learn about stock market ? <eos> what is the best way to learn about stock market ...\n"
     ]
    }
   ],
   "source": [
    "print(ss,\"\\n\")\n",
    "for i in range(30):\n",
    "    n=res[-1].topk(2)[1]\n",
    "    n = n[1] if n.data[0]==0 else n[0]\n",
    "    print(TEXT.vocab.itos[n.data[0]], end=' ')\n",
    "    res,*_ = m(n[0].unsqueeze(0))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "     38     37     25  ...       5      2    122\n",
       "   4504     40  24961  ...      25      3     48\n",
       "     17  71798    332  ...   19329     20     10\n",
       "         ...            ⋱           ...         \n",
       "    451      2   1223  ...       3    187      5\n",
       "  19144      3      9  ...       8     57      6\n",
       "      2      8    591  ...      16   4681      4\n",
       " [torch.cuda.LongTensor of size 64x16 (GPU 0)], Variable containing:\n",
       "   4504\n",
       "     40\n",
       "  24961\n",
       "   ⋮   \n",
       "      7\n",
       "    102\n",
       "     23\n",
       " [torch.cuda.LongTensor of size 1024 (GPU 0)])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(learner.data.trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8282"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learner.data.trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = next(iter(learner.data.trn_dl))[0][:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(83932, 200, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(83932, 200, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(200, 500, dropout=0.05)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(500, 500, dropout=0.05)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(500, 200, dropout=0.05)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=200, out_features=83932, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace the LinearDecoder with the Siamese model defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model[0](learner.data.trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(Residual, self).__init__()\n",
    "        drate = .3\n",
    "        self.math = nn.Sequential(\n",
    "                                 nn.BatchNorm1d(insize),\n",
    "                                 nn.Dropout(drate),\n",
    "                                 nn.Linear(insize, outsize),\n",
    "                                 nn.PReLU(),\n",
    "                                )\n",
    "        self.skip = nn.Linear(insize, outsize)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.math(x) + self.skip(x)\n",
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        # this is final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "                            Residual(4*(128), 256),\n",
    "                            Residual(256, 128),\n",
    "                            Residual(128, 128),\n",
    "                            nn.Linear(128, 1),\n",
    "                            nn.Sigmoid()\n",
    "                            )\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        z = torch.cat([x*y, (x-y)**2], 1)\n",
    "        z = self.classifier(z)\n",
    "        return z\n",
    "    \n",
    "test_model = Siamese()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would need some more time to complete this. \n",
    "\n",
    "RNN_encoder model needs to be added to the Siamese model with frozen weights and then the classifier will be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
